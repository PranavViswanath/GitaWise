{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PranavViswanath/GitaWise/blob/main/submission.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CT6Ci5F2YY0U"
      },
      "source": [
        "# Introduction to Neural Networks - PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jRz98F4_0br"
      },
      "source": [
        "Neural networks are a core component of deep learning, driving advancements in image recognition, natural language processing, robotics, and more. These networks are made up of interconnected layers that transform data through a series of operations, allowing them to learn complex patterns. At the heart of neural networks is the process of learning through gradients, calculated by *backpropagation* and the *chain rule*, which enables the network to adjust its parameters based on how much each parameter contributes to the overall error.\n",
        "\n",
        "Modern deep learning frameworks like **PyTorch** and **TensorFlow** provide powerful abstractions for building and training neural networks, automatically handling gradient calculations and optimization steps. In this part of the assignment, we'll be using PyTorch!\n",
        "\n",
        "In this part of the assignment, we will build a mini neural network framework that supports gradient propagation, allowing us to train a network on a real task and observe how it learns. Specifically, you will:\n",
        "\n",
        "1. **Learn how to handle data inputs**: You will first implement some helper functions that will help you properly process input data and labels for training and evaluation.\n",
        "3. **Implement a Neural Network for Digit Classification (MNIST) using PyTorch**: After building a neural network to solve the XOR problem from scratch, you’ll implement another neural network in PyTorch to experience firsthand how these frameworks abstract many of the complex tasks involved in gradient calculations to solve a classic problem: the MNIST digit classification task. You will also get to tinker with your trained model to see how it performs on some drawings that you'll make!\n",
        "3. **Train and Evaluate on the [Rig Juice](https://regularshow.fandom.com/wiki/Rig_Juice) Classification Task**: Finally, you will train both your custom implementation and the PyTorch version on a *Rig juice classification task*, exploring how each approach learns to classify samples of Rig juice based on a variety of features. This task will help you understand the practical differences between manual implementation and using a modern framework.\n",
        "\n",
        "**Notes:**\n",
        "- We've provided all the imports that you'll need for this assignment, though you're welcome to add imports _from the Python Standard Library only_ if you need them. If you want to add any helper functions for whatever reason, please make sure that they are nested inside the function that they are used in. This is to ensure that the autograder can run your code without any issues.\n",
        "- ***DO NOT REMOVE ANY COMMENTS THAT HAVE `# export` IN THEM. THE GRADING SCRIPT USES THESE COMMENTS TO EVALUATE YOUR FUNCTIONS. WE WILL NOT AUDIT SUBMISSIONS TO ADD THESE. IF THE AUTOGRADER FAILS TO RUN DUE TO YOUR MODIFICATION OF THESE COMMENTS, YOU WILL NOT RECEIVE CREDIT.***\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urY3KHsq_0bs"
      },
      "source": [
        "# Installations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nqv5CDQ8_0bt",
        "outputId": "1ab0f65e-9317-42c2-d32e-c5972ee09eda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m58.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m74.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: torch==2.6.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.6.0+cu124)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.6.0->torchvision) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.6.0->torchvision) (3.0.2)\n",
            "Collecting gradio\n",
            "  Downloading gradio-5.26.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting aiofiles<25.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.9.0 (from gradio)\n",
            "  Downloading gradio_client-1.9.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting groovy~=0.1 (from gradio)\n",
            "  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.30.2)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.16)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.1.0)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.3)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.9.3 (from gradio)\n",
            "  Downloading ruff-0.11.7-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.2)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.13.2)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.34.2-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.9.0->gradio) (2025.3.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.9.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.8)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-5.26.0-py3-none-any.whl (46.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.9/46.9 MB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.9.0-py3-none-any.whl (322 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.6/322.6 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading groovy-0.1.2-py3-none-any.whl (14 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.11.7-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m108.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.46.2-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading uvicorn-0.34.2-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub, uvicorn, tomlkit, semantic-version, ruff, python-multipart, groovy, ffmpy, aiofiles, starlette, safehttpx, gradio-client, fastapi, gradio\n",
            "Successfully installed aiofiles-24.1.0 fastapi-0.115.12 ffmpy-0.5.0 gradio-5.26.0 gradio-client-1.9.0 groovy-0.1.2 pydub-0.25.1 python-multipart-0.0.20 ruff-0.11.7 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.46.2 tomlkit-0.13.2 uvicorn-0.34.2\n"
          ]
        }
      ],
      "source": [
        "%pip install numpy\n",
        "%pip install pandas\n",
        "%pip install matplotlib\n",
        "%pip install torch\n",
        "%pip install torchvision\n",
        "%pip install gradio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72_ombXDbd5g"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Z62QX-pqunOY"
      },
      "outputs": [],
      "source": [
        "# export - DO NOT MODIFY THIS CELL\n",
        "# General imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import io\n",
        "import matplotlib.pyplot as plt\n",
        "from helpers import plot_loss\n",
        "from PIL import Image\n",
        "import gradio as gr\n",
        "\n",
        "# PyTorch imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "from torchvision import datasets, transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d7bg064S_0bv"
      },
      "outputs": [],
      "source": [
        "# export - DO NOT MODIFY OR MOVE THIS LINE\n",
        "# add any additional imports here (from the Python Standard Library only!)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_E-tgGnYY0e"
      },
      "source": [
        "# Neural Networks in PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTFZnTrOYY0e"
      },
      "source": [
        "Now that you have a solid foundation for understanding how the ins and outs of a neural network functions, in this section, we’ll use PyTorch (an industry-standard library) to create a neural network. PyTorch provides predefined layers, optimizers, and utilities that make network training more efficient and flexible.\n",
        "\n",
        "This part of the assignment will allow you to explore the difference in setup and convenience between implementing a network from scratch and using a framework like PyTorch. Specifically, we'll be using PyTorch to implement neural networks to solve two different problems."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7i1Os-X6_0bx"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIBvK3I0_0bx"
      },
      "source": [
        "Before training our neural networks for either problem, we first have to prepare the data by shuffling it and splitting it into training and testing sets.\n",
        "\n",
        "The training set is used to train the model, allowing it to learn the underlying patterns and relationships within the data. In contrast, the testing set serves as a separate dataset that the model has not seen during training. This separation is critical for evaluating the model's performance and its ability to generalize.\n",
        "\n",
        "By using a testing set, we can assess how well the model can make predictions on new, unseen data, which is a key indicator of its effectiveness in real-world applications. Additionally, shuffling the data helps to ensure that the training and testing sets are representative of the overall dataset, reducing the risk of bias and improving the model.\n",
        "\n",
        "Implement the below helper functions to load and prepare the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2EY4H8EE_0bx"
      },
      "source": [
        "### Shuffling the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Vk0MpJT_0by"
      },
      "source": [
        "Shuffling ensures that the data is mixed randomly, which helps the model learn more effectively.\n",
        "\n",
        "Implement the below function to shuffle the data. Hint: You may find [torch.randperm](https://pytorch.org/docs/stable/generated/torch.randperm.html) useful for this function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5L1BOB82_0by"
      },
      "outputs": [],
      "source": [
        "# export - DO NOT MODIFY OR MOVE THIS LINE\n",
        "def shuffle_data(features: torch.Tensor, labels: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
        "    '''\n",
        "    Shuffle the features and labels of a dataset.\n",
        "    Returns a tuple of shuffled features and shuffled labels.\n",
        "    '''\n",
        "    # Generate random permutation indices\n",
        "    indices = torch.randperm(features.size(0))\n",
        "\n",
        "    # Use these indices to shuffle both features and labels\n",
        "    shuffled_features = features[indices]\n",
        "    shuffled_labels = labels[indices]\n",
        "\n",
        "    return shuffled_features, shuffled_labels\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uh6J-njB_0by"
      },
      "source": [
        "### Splitting the Data into Training and Testing Sets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inbfg3di_0by"
      },
      "source": [
        "Once the data is shuffled, we can split it into training and testing sets. The training set will be used to train the model, while the testing set will be used to evaluate its performance. The split ratio is typically 80% for training and 20% for testing. Though this is something you can tune, we've set this ratio for you to keep things simple.\n",
        "\n",
        "Implement the below helper function to split the data into training and testing sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MjhaX3lg_0by"
      },
      "outputs": [],
      "source": [
        "# export - DO NOT MODIFY OR MOVE THIS LINE\n",
        "def split_data(features: torch.Tensor, labels: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "    '''\n",
        "    Splits the data into a train and test set using\n",
        "    the defined constant, TRAIN_TEST_SPLIT_RATIO.\n",
        "    '''\n",
        "    TRAIN_TEST_SPLIT_RATIO = 0.8\n",
        "\n",
        "    # Calculate the split point\n",
        "    split_idx = int(features.size(0) * TRAIN_TEST_SPLIT_RATIO)\n",
        "\n",
        "    # Split the features and labels\n",
        "    train_features = features[:split_idx]\n",
        "    train_labels = labels[:split_idx]\n",
        "    test_features = features[split_idx:]\n",
        "    test_labels = labels[split_idx:]\n",
        "\n",
        "    return train_features, train_labels, test_features, test_labels\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBH1OL8w_0bz"
      },
      "source": [
        "### Creating the Dataset and DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qu2FQqbY_0bz"
      },
      "source": [
        "PyTorch has two very useful abstractions, Dataset and DataLoader, to simplify how we handle data during training.\n",
        "- `Dataset`: An abstract class representing a dataset. Custom datasets inherit from this class and must implement __len__() and __getitem__(). For common use cases like tensors, PyTorch offers ready-made implementations such as `TensorDataset`, which is what we'll use. This class allows you to store inputs and their corresponding labels as tensors, making them easy to access and manipulate.\n",
        "- `DataLoader`: Combines a dataset with an iterable. It handles batching, shuffling, and parallel loading with minimal code. This is _especially_ useful for feeding data into a model efficiently during training and evaluation.\n",
        "\n",
        "Your task is to implement the below function to create a data loader using the PyTorch helper functions. We'll eventually use this data loader to feed data into our neural networks during training and evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8IVnd8pd_0bz"
      },
      "outputs": [],
      "source": [
        "# export - DO NOT MODIFY OR MOVE THIS LINE\n",
        "def create_dataloader(train_features: torch.Tensor, train_labels: torch.Tensor, batch_size: int = 64) -> DataLoader:\n",
        "    \"\"\"Creates a DataLoader for the training features and labels.\"\"\"\n",
        "    # Create a TensorDataset from features and labels\n",
        "    dataset = TensorDataset(train_features, train_labels)\n",
        "\n",
        "    # Create a DataLoader from the dataset\n",
        "    train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    return train_loader\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GO5SZ_rn_0bz"
      },
      "source": [
        "## MNIST Digit Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJhJ2QYB_0bz"
      },
      "source": [
        "### MNIST Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "do59jnmR_0bz"
      },
      "source": [
        "Now that we've written helper methods to prepare the data, let's move onto the first problem: **MNIST Digit Classification**. In this problem, we will build a neural network to classify handwritten digits from the MNIST dataset. The MNIST dataset is a well-known benchmark in the field of machine learning and computer vision, consisting of 70,000 grayscale images of handwritten digits (0-9) and their corresponding labels. Each image is 28x28 pixels in size.\n",
        "\n",
        "We’ll load in the data and convert it into [PyTorch tensors](https://pytorch.org/tutorials/beginner/introyt/tensors_deeper_tutorial.html), which are the primary data structures used in PyTorch for building and training neural networks.\n",
        "\n",
        "Before diving into training, let's visualize some examples from the dataset to better understand the kind of data we'll be working with. Run the below cell to see some of the images!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LUms6PQj_0bz"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "mnist_dataset = datasets.MNIST('data', train=True, download=True, transform=transform)\n",
        "\n",
        "\n",
        "sample_indices = np.random.randint(0, len(mnist_dataset), size=5)\n",
        "fig, axs = plt.subplots(1, 5, figsize=(15, 3))\n",
        "fig.suptitle('Sample MNIST Images')\n",
        "for i, idx in enumerate(sample_indices):\n",
        "    image, label = mnist_dataset[idx]\n",
        "    axs[i].imshow(image.squeeze(), cmap='gray')\n",
        "    axs[i].set_title(f'Label: {label}')\n",
        "    axs[i].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "images = torch.stack([img for img, _ in mnist_dataset])\n",
        "labels = torch.tensor([label for _, label in mnist_dataset])\n",
        "\n",
        "print(f\"Number of Samples: {len(images)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2EMAlaEm_0b0"
      },
      "source": [
        "### Building Your Neural Network to Classify Digits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gnAk6w5_0b0"
      },
      "source": [
        "Now, let's create our own neural network for the MNIST problem using PyTorch. For this problem, your network can have any number of layers and nodes, as long as it is set up for **classification** (not regression).\n",
        "\n",
        "Here are some core PyTorch components we recommend you use to build and train your model (click the hyper links to learn more about each component):\n",
        "\n",
        "1. [**`nn.Module`**](https://pytorch.org/docs/stable/generated/torch.nn.Module.html): The base class for all neural network modules in PyTorch. By subclassing `nn.Module`, you can define and structure your own neural networks. Each layer of the network (like fully connected layers or activation functions) will be an attribute of this module, and the `forward()` method will define the data flow through these layers.\n",
        "\n",
        "2. [**`nn.Linear`**](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html): This layer applies a linear transformation to the incoming data, making it useful for fully connected (dense) layers in a neural network.\n",
        "\n",
        "3. [**`nn.Sigmoid`**](https://pytorch.org/docs/stable/generated/torch.nn.Sigmoid.html): A common activation function that maps input values to an output range of `[0, 1]`.\n",
        "\n",
        "4. [**`nn.CrossEntropyLoss`**](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html): This is a loss function suited for classification tasks, especially where classes are mutually exclusive. `nn.CrossEntropyLoss` combines `log_softmax` and `nll_loss` in a single function, which makes it numerically more stable.\n",
        "\n",
        "PyTorch offers a wide range of layers, activation functions, and loss functions for building neural networks, making it a flexible and powerful tool for experimentation. **While the components mentioned above are enough to complete this task, we encourage you to explore more.** There are many other layers, loss functions, and activation functions (some you've learned about, some new!) that may help you solve different types of problems or improve model performance.\n",
        "\n",
        "Check out the full list of [PyTorch's Neural Network Layers and Loss Functions](https://pytorch.org/docs/stable/nn.html)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nmp9lm_z_0b0"
      },
      "outputs": [],
      "source": [
        "# export - DO NOT MODIFY OR MOVE THIS LINE\n",
        "# Initialize the layers, criterion, and model with classification objective\n",
        "class MNISTNeuralNetwork(nn.Module):\n",
        "    def __init__(self, input_size: int, hidden_size: int, output_size: int) -> None:\n",
        "        super(MNISTNeuralNetwork, self).__init__()\n",
        "        # Flatten layer to convert 2D images to 1D\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        # First fully connected layer\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "\n",
        "        # Activation function\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        # Second fully connected layer\n",
        "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # Flatten the input\n",
        "        x = self.flatten(x)\n",
        "\n",
        "        # First layer with activation\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        # Output layer\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lTGkQfS_0b0"
      },
      "source": [
        "### MNIST Hyperparameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91Bcc18C_0b0"
      },
      "source": [
        "Set the following hyperparameters:\n",
        "- `EPOCHS`: Number of times the entire training dataset is passed through the model. This should be a positive integer that is a multiple of 10.\n",
        "- `LR` (Learning Rate): Controls how much to adjust the model weights during training. This should be a positive float.\n",
        "- `BATCH_SIZE`: Number of samples processed before the model is updated. This should be a positive integer that that is a power of 2.\n",
        "- `EVAL_EVERY`: Frequency (in epochs) at which to evaluate the model on the validation set. This can be any positive integer.\n",
        "    \n",
        "Feel free to experiment with different values to see how they affect training. We've put in placeholders as default values which might not be the best for your model, so feel free to change them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8lJgcKob_0b0"
      },
      "outputs": [],
      "source": [
        "# export - DO NOT MODIFY OR MOVE THIS LINE\n",
        "def set_mnist_hyperparameters() -> None:\n",
        "    '''\n",
        "    Sets the hyperparameters for the MNIST model.\n",
        "    '''\n",
        "    global EPOCHS, LR, BATCH_SIZE, EVAL_EVERY\n",
        "\n",
        "    EPOCHS = 20\n",
        "    LR = 0.1\n",
        "    BATCH_SIZE = 128\n",
        "    EVAL_EVERY = 2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h980ViMv_0b0"
      },
      "outputs": [],
      "source": [
        "set_mnist_hyperparameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27r0-jwn_0b1"
      },
      "source": [
        "### MNIST PyTorch Training Loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1J8DgwL_0b1"
      },
      "source": [
        "Now that we have defined our PyTorch model, let’s set up a training loop to optimize the model parameters. This loop will look very similar to the MicroTorch training loop but with PyTorch-specific syntax.\n",
        "\n",
        "**Key Components:**\n",
        "\n",
        "1. **Optimizer**: PyTorch provides a variety of optimizers for updating model parameters based on computed gradients. Here, we will use `optim.SGD` for stochastic gradient descent. The optimizer is initialized with the model parameters and learning rate.\n",
        "   - [PyTorch Documentation for `optim.SGD`](https://pytorch.org/docs/stable/optim.html#torch.optim.SGD)\n",
        "\n",
        "2. **`.backward()`**: This method computes the gradient of the loss with respect to each parameter using backpropagation. PyTorch handles all gradient calculations automatically, so you don’t need to manually derive them as in MicroTorch.\n",
        "\n",
        "3. **`.zero_grad()`**: Before each backward pass, we need to reset the gradients for each parameter by calling `optimizer.zero_grad()`. This ensures that gradients from previous steps don’t accumulate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gETUNstL_0b1"
      },
      "outputs": [],
      "source": [
        "# export - DO NOT MODIFY OR MOVE THIS LINE\n",
        "def train_model_pytorch_mnist(images, labels, plot_losses=True):\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    # Initialize model, loss, optimizer, and set the hyperparameters\n",
        "    ### YOUR CODE BELOW HERE ###\n",
        "    # Create model instance with appropriate dimensions for MNIST\n",
        "    # MNIST images are 28x28 = 784 input features\n",
        "    set_mnist_hyperparameters()\n",
        "\n",
        "    input_size = 28 * 28\n",
        "    hidden_size = 128\n",
        "    output_size = 10  # 10 digits (0-9)\n",
        "\n",
        "    model = MNISTNeuralNetwork(input_size, hidden_size, output_size)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=LR)\n",
        "    ### YOUR CODE ABOVE HERE ###\n",
        "\n",
        "    # Create DataLoaders\n",
        "    images, labels = shuffle_data(images, labels)\n",
        "    train_features, train_labels, test_features, test_labels = split_data(\n",
        "        images, labels\n",
        "    )\n",
        "    train_loader = create_dataloader(train_features, train_labels, batch_size=BATCH_SIZE)\n",
        "    test_loader = create_dataloader(test_features, test_labels, batch_size=BATCH_SIZE)\n",
        "\n",
        "    for epoch in range(EPOCHS + 1):\n",
        "        model.train()\n",
        "        total_train_loss = 0\n",
        "        \"\"\"\n",
        "        Implement the training loop steps:\n",
        "        1. Forward pass through the model\n",
        "        2. Calculate loss\n",
        "        3. Zero gradients\n",
        "        4. Backward pass\n",
        "        5. Update parameters\n",
        "        \"\"\"\n",
        "        for batch_features, batch_labels in train_loader:\n",
        "            ### YOUR CODE BELOW HERE ###\n",
        "            # Forward pass\n",
        "            outputs = model(batch_features)\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = criterion(outputs, batch_labels)\n",
        "\n",
        "            # Zero gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "\n",
        "            # Update parameters\n",
        "            optimizer.step()\n",
        "            ### YOUR CODE ABOVE HERE ###\n",
        "            total_train_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = total_train_loss / len(train_loader)\n",
        "        train_losses.append(avg_train_loss)\n",
        "\n",
        "        # Evaluate on validation set every EVAL_EVERY epochs\n",
        "        if epoch % EVAL_EVERY == 0:\n",
        "            model.eval()\n",
        "            total_val_loss = 0\n",
        "            total_correct = 0\n",
        "\n",
        "            \"\"\"\n",
        "            Implement the evaluation loop steps:\n",
        "            1. Forward pass through the model (no need for gradients)\n",
        "            2. Calculate loss\n",
        "            3. Get predictions and compare with true labels\n",
        "            \"\"\"\n",
        "            with torch.no_grad():\n",
        "                for batch_features, batch_labels in test_loader:\n",
        "                    ### YOUR CODE BELOW HERE ###\n",
        "                    # Forward pass\n",
        "                    outputs = model(batch_features)\n",
        "\n",
        "                    # Calculate loss\n",
        "                    loss = criterion(outputs, batch_labels)\n",
        "                    total_val_loss += loss.item()\n",
        "\n",
        "                    # Get predictions and calculate accuracy\n",
        "                    predictions = torch.argmax(outputs, dim=1)\n",
        "                    total_correct += (predictions == batch_labels).sum().item()\n",
        "                    ### YOUR CODE ABOVE HERE ###\n",
        "\n",
        "            avg_val_loss = total_val_loss / len(test_loader)\n",
        "            accuracy = (total_correct / len(test_loader.dataset)) * 100\n",
        "            val_losses.append(avg_val_loss)\n",
        "\n",
        "            print(\n",
        "                f\"Epoch {epoch}/{EPOCHS} | \"\n",
        "                f\"Train Loss: {avg_train_loss:.4f} | \"\n",
        "                f\"Validation Loss: {avg_val_loss:.4f} | \"\n",
        "                f\"Accuracy: {accuracy:.2f}%\"\n",
        "            )\n",
        "\n",
        "            if plot_losses:\n",
        "                plot_loss(\n",
        "                    train_losses, val_losses, eval_interval=EVAL_EVERY, dynamic=True\n",
        "                )\n",
        "\n",
        "    print(\n",
        "        f\"Final Train Loss: {avg_train_loss:.4f} | \"\n",
        "        f\"Final Validation Loss: {avg_val_loss:.4f} | \"\n",
        "        f\"Final Accuracy: {accuracy:.2f}%\"\n",
        "    )\n",
        "    if plot_losses:\n",
        "        plot_loss(train_losses, val_losses, eval_interval=EVAL_EVERY, dynamic=False)\n",
        "    return model, train_losses, val_losses\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tQcLb8xP_0b1"
      },
      "outputs": [],
      "source": [
        "mnist_model, train_losses, val_losses = train_model_pytorch_mnist(images, labels, plot_losses=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPLJhO-K_0b1"
      },
      "source": [
        "**You should achieve an accuracy of above 95% in this portion.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q71mkAHZ_0b1"
      },
      "outputs": [],
      "source": [
        "mnist_test_dataset = datasets.MNIST('data', train=False, download=True, transform=transform)\n",
        "test_loader = DataLoader(mnist_test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "print(f\"Number of test samples: {len(test_loader.dataset)}\")\n",
        "\n",
        "total_correct = 0\n",
        "mnist_model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for test_samples, test_labels in test_loader:\n",
        "        test_outputs = mnist_model(test_samples)\n",
        "        predicted_labels = torch.argmax(test_outputs, dim=1)\n",
        "        total_correct += (predicted_labels == test_labels).sum().item()\n",
        "\n",
        "accuracy = (total_correct / len(test_loader.dataset)) * 100\n",
        "print(f'Accuracy: {accuracy:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7P8qvGcC_0b1"
      },
      "source": [
        "### Hand Drawing some MNIST Inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJyz3kqr_0b2"
      },
      "source": [
        "If you'd like to test your code further, we encourage you to draw some digits using the below cell. It will create a sketchpad for you to draw digits, and then it will use your trained model to predict the digit you drew.\n",
        "\n",
        "Note that this will not be graded and is purely for your own exploration. Results may vary, as the images that you are creating are not exactly the same as the MNIST dataset. However, it should be a fun way to see how your model performs on new data!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "smhtrrDg_0b6"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((28, 28)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "def predict_digit(image_dict: dict) -> str:\n",
        "    '''\n",
        "    Converts the input image to a tensor and predicts the digit.\n",
        "    Returns the predicted digit as a string.\n",
        "    '''\n",
        "    image = image_dict[\"composite\"]\n",
        "\n",
        "    # Uncomment the below line to see the raw input image for debugging\n",
        "    # Image.fromarray(image).save(\"raw_input.png\")\n",
        "\n",
        "    # RGBA images\n",
        "    if image.shape[2] == 4:\n",
        "        # Extract the alpha channel\n",
        "        alpha = image[..., 3]\n",
        "\n",
        "        # Create a mask where alpha > 0 (something's drawn here)\n",
        "        mask = alpha > 0\n",
        "\n",
        "        # For MNIST, we want black background (0) and white digits (255)\n",
        "        result = np.zeros_like(alpha) # Black background\n",
        "        result[mask] = 255 # Set drawn areas to white (255)\n",
        "    else:\n",
        "        # If RGB only, convert to grayscale\n",
        "        gray = np.mean(image, axis=2).astype(np.uint8)\n",
        "        result = gray\n",
        "\n",
        "    # Convert to PIL and save/debug\n",
        "    image_pil = Image.fromarray(result)\n",
        "\n",
        "    # Uncomment the below line to see the processed image for debugging\n",
        "    # image_pil.save(\"digit_debug.png\")\n",
        "\n",
        "    # Apply transforms\n",
        "    digit = transform(image_pil).unsqueeze(0)\n",
        "\n",
        "    # Save the tensor image for debugging\n",
        "    transformed_image = digit.squeeze().numpy()\n",
        "\n",
        "    # Properly reverse the normalization for visualization\n",
        "    vis_img = ((transformed_image * 0.5 + 0.5) * 255).astype(np.uint8)\n",
        "\n",
        "    # Uncomment the below line to see the input image for the model for debugging\n",
        "    Image.fromarray(vis_img).save(\"model_input.png\")\n",
        "\n",
        "    mnist_model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = mnist_model(digit)\n",
        "        pred = output.argmax(dim=1).item()\n",
        "\n",
        "    return f\"Predicted Digit: {pred}\"\n",
        "\n",
        "app = gr.Interface(fn=predict_digit, inputs=\"sketchpad\", outputs=\"text\", live=False)\n",
        "app.launch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15FqW6_5_0b6"
      },
      "outputs": [],
      "source": [
        "# run this to stop the app connection to the server in the notebook\n",
        "app.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t09sl2LU_0b6"
      },
      "source": [
        "## Rig Juice Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_y_pH0T_0b6"
      },
      "source": [
        "When wandering the vast expanse of TextWorld, you've encountered a special power-up, courtesy of your good friend, Rigby: **RIG JUICE**. It's a strange, vibrant liquid that seems to have a mind of its own. It can be used to power up your search through TextWorld, but it also has a tendency to cause the agent to explode if it's unsafe. Luckily, you've been given a dataset with the characteristics of the different possible Rig Juice flavors (Rigby tends to invent new flavors on the fly).\n",
        "\n",
        "\n",
        "Rig juice is a juice that contains random items that Rigby finds, and since its conception, it has become a highly-valued substance in the TextWorld economy. Although little is known about the makeup of each jug of Rig juice found, they have unique ingredients and have been made with mysterious processes that make them highly sought after by explorers. For more information about Rig juice and its wondrous effects, [check out the following link](https://regularshow.fandom.com/wiki/Rig_Juice).\n",
        "\n",
        "Because they can be made with some wild ingredients, we need to make sure that the Rig juice that we're dealing with is safe. Many health experts in the TextWorld community have found poisonous jugs of Rig juice in the wild (Rigby's a careless guy sometimes). Our task is to use a neural network to determine whether a given jug of Rig juice is **poisonous** or **non-poisonous**, to ensure that our agent doesn't get injured in their journey. By training our neural network on this dataset, we want to achieve a high accuracy in a [binary classification](https://www.learndatasci.com/glossary/binary-classification/) of Rig juice. This will help improve our understanding the uniqueness of Rig juice to harness its utility to effectively search through TextWorld.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDU2F4Yc_0b6"
      },
      "source": [
        "### Setting up the Rig Juice Dataset and Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zG-RC_8_0b7"
      },
      "source": [
        "First, let's load the dataset and prepare the features and labels for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "urxe0NuR_0b7"
      },
      "outputs": [],
      "source": [
        "dataset = pd.read_csv(\"rig_juice_student.csv\")\n",
        "dataset.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_qJ5YYh_0b7"
      },
      "source": [
        "These are the dataset columns (features)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "50scYgqC_0b7"
      },
      "outputs": [],
      "source": [
        "print(\"Dataset columns:\")\n",
        "print(dataset.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KObzJnC7_0b7"
      },
      "source": [
        "The dataset contains various features (attributes; in this case, ingredients) of a sample of Rig Juice and a label indicating their class. The first column represents the label (class) which'll tell us whether something is poisonous or not, and the remaining columns are the features.\n",
        "\n",
        "As we did before, we'll convert the data to [PyTorch tensors](https://pytorch.org/tutorials/beginner/introyt/tensors_deeper_tutorial.html) for ease of use in our neural network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JXmUIltb_0b7"
      },
      "outputs": [],
      "source": [
        "# Convert the data to tensors\n",
        "labels = torch.tensor(dataset[\"poisonous\"].to_numpy(), dtype=torch.long)\n",
        "features = torch.tensor(dataset.drop(columns = [\"poisonous\"]).to_numpy(), dtype=torch.float32)\n",
        "\n",
        "print(f\"Number of samples: {len(features)}\")\n",
        "print(f\"Number of features: {features.shape[1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgjkCUsS_0b7"
      },
      "source": [
        "Now, we'll define the neural network model for our binary classification task of the Rig juice dataset to determine whether samples of the Rig juice are poisonous or not. We'll use PyTorch's `nn.Module` class to create our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zH5o5XRS_0b7"
      },
      "outputs": [],
      "source": [
        "# export - DO NOT MODIFY OR MOVE THIS LINE\n",
        "class RigJuiceNeuralNetwork(nn.Module):\n",
        "    def __init__(self, input_size: int, hidden_size: int, output_size: int) -> None:\n",
        "        super(RigJuiceNeuralNetwork, self).__init__()\n",
        "        # First fully connected layer\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "\n",
        "        # Activation function\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        # Second fully connected layer\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size // 2)\n",
        "\n",
        "        # Output layer\n",
        "        self.fc3 = nn.Linear(hidden_size // 2, output_size)\n",
        "\n",
        "        # Dropout for regularization\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # First layer with activation and dropout\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Second layer with activation\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        # Output layer\n",
        "        out = self.fc3(x)\n",
        "\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFJrfrxE_0b7"
      },
      "source": [
        "### Rig Juice Hyperparameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uj4neubM_0b7"
      },
      "source": [
        "Set the following hyperparameters:\n",
        "- `EPOCHS`: Number of times the entire training dataset is passed through the model. This should be a positive integer that is a multiple of 10.\n",
        "- `LR` (Learning Rate): Controls how much to adjust the model weights during training. This should be a positive float.\n",
        "- `BATCH_SIZE`: Number of samples processed before the model is updated. This should be a positive integer that that is a power of 2.\n",
        "- `EVAL_EVERY`: Frequency (in epochs) at which to evaluate the model on the validation set. This can be any positive integer.\n",
        "    \n",
        "Feel free to experiment with different values to see how they affect training. We've put in placeholders as default values which might not be the best for your model, so feel free to change them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zphbc-yl_0b7"
      },
      "outputs": [],
      "source": [
        "# export - DO NOT MODIFY OR MOVE THIS LINE\n",
        "def set_rig_juice_hyperparameters() -> None:\n",
        "    '''\n",
        "    Sets the hyperparameters for the Rig Juice model.\n",
        "    '''\n",
        "    global EPOCHS, LR, BATCH_SIZE, EVAL_EVERY\n",
        "\n",
        "    EPOCHS = 20\n",
        "    LR = 0.1\n",
        "    BATCH_SIZE = 64\n",
        "    EVAL_EVERY = 5\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hUij6NvC_0b7"
      },
      "outputs": [],
      "source": [
        "set_rig_juice_hyperparameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rxy_ZX62_0b7"
      },
      "source": [
        "### Rig Juice Training Loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0Weu-Vz_0b7"
      },
      "source": [
        "Fill in the missing code to complete the training loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d6po2ufm_0b8"
      },
      "outputs": [],
      "source": [
        "# export - DO NOT MODIFY OR MOVE THIS LINE\n",
        "def train_model_pytorch_rig_juice(features, labels, plot_losses=True):\n",
        "    '''\n",
        "    In this function, you will implement the training loop for the Rig Juice dataset.\n",
        "    The training loop will include the following steps:\n",
        "    - Initialize the model, loss function, and optimizer\n",
        "    - Shuffle the data, split it into training and test sets\n",
        "    - train the model using the training set\n",
        "        - for each batch in each epoch, perform the forward pass, calculate loss, zero gradients, and backward pass\n",
        "    - Evaluate the model using the test set\n",
        "    - Plot the training and validation losses (if you wish)\n",
        "    - Return the model, criterion, optimizer, training losses, validation losses, and test set\n",
        "    '''\n",
        "    # Initialize lists to store loss values\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    # Initialize the model, loss, and optimizer, and set the hyperparameters\n",
        "    # please name these respectively as `model`, `criterion`, and `optimizer`\n",
        "    # YOUR CODE BELOW HERE\n",
        "    set_rig_juice_hyperparameters()\n",
        "\n",
        "    input_size = features.shape[1]\n",
        "    hidden_size = 64\n",
        "    output_size = 2  # Binary classification (poisonous or not)\n",
        "\n",
        "    model = RigJuiceNeuralNetwork(input_size, hidden_size, output_size)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=LR)\n",
        "    # YOUR CODE ABOVE HERE\n",
        "\n",
        "    # preparing the data\n",
        "    features, labels = shuffle_data(features, labels)\n",
        "    train_features, train_labels, test_features, test_labels = split_data(features, labels)\n",
        "    train_loader = create_dataloader(train_features, train_labels, batch_size=BATCH_SIZE)\n",
        "\n",
        "    for i in range(EPOCHS + 1):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for batch_features, batch_labels in train_loader:\n",
        "            \"\"\"\n",
        "            Implement the training loop steps:\n",
        "            1. Forward pass through the model\n",
        "            2. Calculate loss\n",
        "            3. Zero gradients\n",
        "            4. Backward pass\n",
        "            5. Update parameters\n",
        "            \"\"\"\n",
        "            # YOUR CODE BELOW HERE\n",
        "            # Forward pass\n",
        "            outputs = model(batch_features)\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = criterion(outputs, batch_labels)\n",
        "\n",
        "            # Zero gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "\n",
        "            # Update parameters\n",
        "            optimizer.step()\n",
        "            # YOUR CODE ABOVE HERE\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        loss = total_loss / len(train_loader)\n",
        "        train_losses.append(loss)\n",
        "\n",
        "        if i % EVAL_EVERY == 0:\n",
        "            \"\"\"\n",
        "            Implement the evaluation loop steps:\n",
        "            1. Forward pass through the model (no need for gradients)\n",
        "            2. Calculate loss\n",
        "            3. Get predictions and compare with true labels\n",
        "            \"\"\"\n",
        "            # YOUR CODE BELOW HERE\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                outputs = model(test_features)\n",
        "                loss = criterion(outputs, test_labels)\n",
        "\n",
        "                # Calculate accuracy\n",
        "                predictions = torch.argmax(outputs, dim=1)\n",
        "                accuracy = (predictions == test_labels).sum().item() / len(test_labels)\n",
        "\n",
        "                print(f\"Epoch {i}/{EPOCHS} | Train Loss: {train_losses[-1]:.4f} | Val Loss: {loss.item():.4f} | Accuracy: {accuracy*100:.2f}%\")\n",
        "            # YOUR CODE ABOVE HERE\n",
        "            val_losses.append(loss.item())\n",
        "            if plot_losses:\n",
        "                plot_loss(train_losses, val_losses, eval_interval=EVAL_EVERY, dynamic=True)\n",
        "\n",
        "    if plot_losses:\n",
        "        plot_loss(train_losses, val_losses, eval_interval=EVAL_EVERY, dynamic=False)\n",
        "    return model, criterion, optimizer, train_losses, val_losses, (test_features, test_labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "krSfTnSG_0b8"
      },
      "outputs": [],
      "source": [
        "rig_juice_model, criterion, optimizer, train_losses, val_losses, test_data = train_model_pytorch_rig_juice(features, labels, plot_losses=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRQT0Tly_0b8"
      },
      "source": [
        "**You should achieve an accuracy of 95% in this portion.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MkyfexOE_0b8"
      },
      "outputs": [],
      "source": [
        "test_features, test_labels = test_data\n",
        "predictions = rig_juice_model(test_features)\n",
        "predictions = torch.argmax(predictions, dim=1)\n",
        "accuracy = (predictions == test_labels).sum().item() / len(test_labels)\n",
        "print(\"Accuracy: \", str(100 * accuracy) + \"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xrspi1xOyWcB"
      },
      "source": [
        "# Grading\n",
        "\n",
        "You score for this part of the assignment will be out of **50 points**. A more detailed breakdown is as follows:\n",
        "- 5 points will be towards implementing the data loading functions correctly.\n",
        "- 15 points will be towards implementing the MNIST network in PyTorch correctly (achieving 95% accuracy).\n",
        "- 30 points will be towards implementing the Rig Juice network in PyTorch correctly (achieving 95% accuracy).\n",
        "\n",
        "**There will be no hidden tests for the MNIST network or the data loading functions.** Your score for that portion of the assignment will be visible when submitting to Gradescope.\n",
        "\n",
        "For the hidden tests for the Rig Juice dataset, we will use data from the same dataset provided (that is, you will not have to generalize your code to perform on a different dataset) that we have partitioned already. This is to ensure correctness of your implementation. The autograder will run a variety of tests for each section and assign scores, which will be visible after grades are published on Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrRHYKs_yZIU"
      },
      "source": [
        "# Submission\n",
        "Upload this notebook with the name `submission.ipynb` file to Gradescope. The autograder will **only** run successfully if your file is named this way. You must ensure that you have removed all print statements from **your** code, or the autograder may fail to run. Excessive print statements will also result in muddled test case outputs, which makes it more difficult to interpret your score.\n",
        "\n",
        "We've added appropriate comments to the top of certain cells for the autograder to export (`# export`). You do NOT have to do anything (e.g. remove print statements) to cells we have provided - anything related to those have been handled for you. You are responsible for ensuring your own code has no syntax errors or unnecessary print statements. You ***CANNOT*** modify the export comments at the top of the cells, or the autograder will fail to run on your submission.\n",
        "\n",
        "You should ***not*** add any cells that your code requires to the notebook when submitting. You're welcome to add any code as you need to extra cells when testing, but they will not be graded. Only the provided cells will be graded. As mentioned in the top of the notebook, **any helper functions that you add should be nested within the function that uses them.**\n",
        "\n",
        "If you encounter any issues with the autograder, please feel free to make a post on Ed Discussion. We highly recommend making a public post to clarify any questions, as it's likely that other students have the same questions as you! If you have a question that needs to be private, please make a private post.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "CS-3600",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}